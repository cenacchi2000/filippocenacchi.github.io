<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>LLM-Guided Clinical Avatar • Filippo Cenacchi</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description"
        content="A 3D clinical avatar and imaging-guided organ twin prototype that combines Human Reference Atlas meshes, a clinical-twin model and CT/MRI-derived organ evolution.">

  <style>
    :root{
      --bg:#f7f9fc;--panel:#ffffff;--text:#0b1220;--subtle:#475569;
      --brand:#2563eb;--brand-ink:#1d4ed8;--border:#e5e7eb;
      --shadow:0 10px 32px rgba(15,23,42,0.10);
      --accent-soft:rgba(37,99,235,0.08);
    }
    *{box-sizing:border-box;}
    html,body{
      margin:0;padding:0;min-height:100vh;
      font:16px/1.6 system-ui,-apple-system,Segoe UI,Roboto,Ubuntu;
      color:var(--text);
      background:
        radial-gradient(1200px 700px at 10% -10%,rgba(37,99,235,.18),transparent 55%),
        radial-gradient(900px 600px at 100% 0%,rgba(52,211,153,.15),transparent 55%),
        linear-gradient(180deg,#f7f9fc 0%,#fbfdff 55%,#f7f9fc 100%);
    }
    a{color:var(--brand-ink);text-decoration:none}
    a:hover{text-decoration:underline}
    .container{max-width:960px;margin:0 auto;padding:24px 16px 40px;}
    header{
      display:flex;justify-content:space-between;align-items:center;
      gap:12px;margin-bottom:24px;
    }
    .back-link{
      display:inline-flex;align-items:center;gap:6px;
      padding:6px 10px;border-radius:999px;
      background:#fff;border:1px solid var(--border);
      font-size:14px;box-shadow:0 4px 16px rgba(15,23,42,0.12);
    }
    h1{font-size:clamp(30px,4vw,40px);margin:8px 0 4px;}
    .subtitle{margin:0;color:var(--subtle);font-size:15px;}
    .hero{
      display:grid;grid-template-columns:minmax(0,2fr) minmax(260px,1.4fr);
      gap:24px;align-items:flex-start;margin-bottom:28px;
    }
    .hero-media{
      border-radius:22px;overflow:hidden;border:1px solid var(--border);
      box-shadow:var(--shadow);background:#000;
    }
    .hero-media img{width:100%;height:100%;object-fit:cover;display:block;}
    .pill{
      display:inline-flex;align-items:center;gap:8px;
      padding:6px 10px;border-radius:999px;
      background:rgba(15,23,42,0.03);
      border:1px solid rgba(148,163,184,0.6);
      font-size:13px;color:var(--subtle);
    }
    .pill span.dot{width:8px;height:8px;border-radius:999px;background:#22c55e;}
    section{
      margin-bottom:26px;background:var(--panel);border-radius:18px;
      border:1px solid var(--border);box-shadow:var(--shadow);
      padding:18px 20px 18px;
    }
    section h2{margin-top:0;font-size:20px;}
    .tag-row{margin-top:8px;display:flex;flex-wrap:wrap;gap:6px;}
    .tag{
      font-size:12px;padding:4px 9px;border-radius:999px;
      background:var(--accent-soft);color:var(--brand-ink);
      border:1px solid rgba(37,99,235,0.25);
    }
    .two-col{
      display:grid;grid-template-columns:minmax(0,1.2fr) minmax(0,1fr);
      gap:18px;
    }
    @media(max-width:800px){
      .hero{grid-template-columns:1fr;}
      .two-col{grid-template-columns:1fr;}
    }
    .callout{
      margin:0;padding:12px 14px;border-radius:12px;
      background:rgba(34,197,94,0.07);
      border-left:4px solid #22c55e;
      font-size:14px;color:var(--subtle);
    }
    .small{font-size:14px;color:var(--subtle);}

    .demo-button{
      display:inline-flex;align-items:center;gap:8px;
      padding:9px 14px;border-radius:999px;
      background:var(--brand);color:#fff;
      font-size:14px;font-weight:500;
      border:none;box-shadow:var(--shadow);
    }
    .demo-button span.icon{font-size:16px;}
    .demo-button:hover{
      background:var(--brand-ink);
      text-decoration:none;
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <a class="back-link" href="index.html#research">← Back to research overview</a>
      <span class="small">Filippo Cenacchi · LLM-Guided Clinical Avatar</span>
    </header>

    <div class="hero">
      <div>
        <p class="pill">
          <span class="dot"></span>
          3D anatomical twin · Imaging-guided organ evolution
        </p>
        <h1>LLM-Guided Clinical Avatar</h1>
        <p class="subtitle">
          A clinical digital-twin prototype that links a full-body anatomical avatar,
          an interview-driven risk model and CT/MRI-derived organ meshes into one
          coherent, interactive simulation.
        </p>
      </div>
      <figure class="hero-media">
        <img src="twin.png"
             alt="3D clinical avatar showing full body anatomy and blood vasculature">
      </figure>
    </div>

    <section>
      <h2>Live Demo</h2>
      <div class="two-col">
        <div>
          <p>
            The prototype is implemented as a <strong>Streamlit application</strong>
            that runs in three coordinated panels:
          </p>
          <ul class="small">
            <li><strong>Avatar</strong> – a HuBMAP / Human Reference Atlas whole-body model with organ-level focusing.</li>
            <li><strong>Agent interview</strong> – a question–answer loop that builds a structured clinical twin.</li>
            <li><strong>Organ progression</strong> – CT/MRI-based meshes with simulated future morphology.</li>
          </ul>
          <p class="small">
            Under the hood the app uses <code>nibabel</code>, <code>scikit-image</code>,
            <code>scipy.ndimage</code> and <code>plotly</code> to go from raw volumetric data
            to risk-coloured 3D organs and future deformations.
          </p>
        </div>
        <div>
          <a class="demo-button"
             href="https://huggingface.co/spaces/filo-cenacchi/SimClinician"
             target="_blank" rel="noopener">
            <span class="icon">▶</span>
            <span>Open live demo on Hugging Face</span>
          </a>
          <p class="small" style="margin-top:10px;">
            ⚠️ Research prototype only — not a medical device and not intended for clinical use.
          </p>
          <div class="tag-row">
            <span class="tag">3D Clinical Avatar</span>
            <span class="tag">Human Reference Atlas</span>
            <span class="tag">Digital Twins</span>
            <span class="tag">CT / MRI Volumes</span>
          </div>
        </div>
      </div>
    </section>

    <section>
      <h2>Architecture at a Glance</h2>
      <div class="two-col">
        <div>
          <p>
            The underlying Python code exposes a clean separation between three layers:
          </p>
          <p class="small">
            <strong>1. Clinical twin model.</strong> A dataclass-based representation
            (<code>ClinicalTwin</code>) encodes demographics, lifestyle and organ-specific
            risk indices. It computes a composite global health score that can be updated
            either manually in the sidebar or via an interview agent.
          </p>
          <p class="small">
            <strong>2. Anatomical avatar layer.</strong> Generic male/female
            GLB bodies and Human Reference Atlas organ meshes are loaded and cached.
            A custom <code>&lt;model-viewer&gt;</code> wrapper renders whole body
            plus an optional focus organ in synchronised viewports.
          </p>
          <p class="small">
            <strong>3. Imaging layer.</strong> A longitudinal imaging database
            (CSV + volumes) is wrapped in an <code>ImagingDatabase</code> class that
            yields time-series of CT/MRI scans per patient and organ, ready to be
            meshed and visualised.
          </p>
        </div>
        <div>
          <p class="small"><strong>Technically, the prototype demonstrates:</strong></p>
          <p class="small">
            • loading real or synthetic volumes (<code>.npy</code>, <code>.nii</code>)<br>
            • intensity-based organ risk maps and morphological evolution<br>
            • marching-cubes meshes via <code>skimage.measure.marching_cubes</code><br>
            • future deformation of high-risk areas at mesh level<br>
            • a self-contained UI that runs both locally and on Hugging Face Spaces
          </p>
        </div>
      </div>
    </section>

    <section>
      <h2>1. Anatomical Avatar & HRA Organ Focusing</h2>
      <div class="two-col">
        <div>
          <p>
            The avatar tab renders a generic male or female anatomical reference
            (GLB) and lets the user “dock” any organ mesh from the
            <strong>Human Reference Atlas</strong> alongside it.
          </p>
          <p class="small">
            Organs are fetched via the HRA API, downloaded once into a local cache and
            exposed through a grouped selector (e.g., kidneys, lungs, liver with
            left/right variants). A lightweight static HTTP server serves GLBs to
            <code>&lt;model-viewer&gt;</code> with proper CORS headers, keeping the
            Streamlit message size small.
          </p>
        </div>
        <div>
          <p class="small"><strong>Contribution.</strong></p>
          <p class="small">
            This layer turns static HRA assets into an <strong>interactive clinical
            front-end</strong> where the same organ can be explored both as a generic
            reference and, in the organ tab, as a patient-specific mesh extracted
            from imaging data. It establishes the “avatar shell” for the digital twin.
          </p>
        </div>
      </div>
    </section>

    <section>
      <h2>2. Interview-Driven Clinical Twin</h2>
      <div class="two-col">
        <div>
          <p>
            The agent tab runs a small question–answer loop that progressively fills
            the clinical twin. In the current prototype this is implemented as a
            deterministic rule-based agent, but the interface is designed to be
            drop-in compatible with a local LLaMA-style LLM.
          </p>
          <p class="small">
            Answers about age, height, weight, smoking status and self-reported
            activity/diet/sleep scores are parsed into structured fields. From these,
            the model computes BMI and adjusts organ-level risks (e.g., lungs and heart
            for smoking and BMI deviations, skeleton and muscle for underweight).
          </p>
        </div>
        <div>
          <p class="small"><strong>Contribution.</strong></p>
          <p class="small">
            The interview layer shows how a conversational agent can be wired directly
            into a clinical twin: every response has an explicit effect on the organ
            risk vector and on the global health score. This offers a concrete
            blueprint for <strong>LLM-mediated parameterisation</strong> of digital
            twins, beyond free-text notes.
          </p>
        </div>
      </div>
    </section>

    <section>
      <h2>3. Imaging-Driven Organ Progression</h2>
      <div class="two-col">
        <div>
          <p>
            The organ progression tab connects the avatar to volumetric imaging.
            A CSV index (<code>patients_index.csv</code>) maps
            <em>(patient, organ, year)</em> tuples to volume paths and optional
            segmentation masks. The demo ships with a synthetic patient
            (DEMO001, lungs + liver, baseline and 5-year follow-up).
          </p>
          <p class="small">
            For each timepoint the pipeline:
          </p>
          <ul class="small">
            <li>loads the CT-like volume and mask,</li>
            <li>computes an organ-specific intensity deviation risk map,</li>
            <li>extracts a surface mesh with marching cubes,</li>
            <li>samples per-vertex risk values to colour the mesh.</li>
          </ul>
          <p class="small">
            Future years beyond the last scan are simulated by deforming the mesh
            outward in high-risk regions and scaling risk values, yielding a
            qualitative view of “worsening” or “improvement” scenarios.
          </p>
        </div>
        <div>
          <p class="small"><strong>Contribution.</strong></p>
          <p class="small">
            This layer grounds the digital twin in <strong>actual imaging
            data</strong>. The same organ is viewable as:
          </p>
          <ul class="small">
            <li>a CT slice with overlayed segmentation,</li>
            <li>a 3D risk-coloured mesh at each timepoint,</li>
            <li>a simulated future trajectory under different clinical assumptions.</li>
          </ul>
          <p class="small">
            The design explicitly separates data loading, risk mapping and
            deformation, making it straightforward to swap in real hospital
            cohorts or disease-specific models.
          </p>
        </div>
      </div>
    </section>

    <section>
      <h2>Research Agenda & Integration</h2>
      <p class="callout">
        The prototype is not about clinical accuracy yet; it is about showing
        <strong>how a humanoid/LLM stack can be wired into a multi-scale
        clinical twin</strong> that spans lifestyle, anatomy and imaging.
      </p>
      <p>
        The same abstractions are planned to be connected to Ameca-based
        humanoid interactions and to the SimClinician framework, allowing
        behavioural signals (facial micro-dynamics, gaze, prosody) and imaging
        to co-exist inside one coherent twin representation.
      </p>
      <p>
        Next steps include replacing heuristic risk maps with model-based ones,
        adding uncertainty estimates, and exploring differential privacy and
        formal de-identification for the imaging and behavioural layers.
      </p>
    </section>
  </div>
</body>
</html>
