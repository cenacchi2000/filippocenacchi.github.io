<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Passive Dementia Screening – Filippo Cenacchi</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- keep if you later move shared CSS here -->
  <link rel="stylesheet" href="styles.css" />

  <!-- local UI styling so this page looks like the main site even if styles.css is empty -->
  <style>
    :root{
      --bg: #f7f9fc;
      --panel: #ffffff;
      --text: #0b1220;
      --subtle: #475569;
      --brand: #2563eb;
      --brand-ink: #1d4ed8;
      --muted: #f1f5f9;
      --border: #e5e7eb;
      --shadow: 0 8px 30px rgba(2,6,23,0.08);
    }

    *{box-sizing:border-box;}

    body.subpage{
      margin:0;
      padding:0;
      background:
        radial-gradient(1200px 700px at 12% -10%, rgba(37,99,235,.12), transparent 50%),
        radial-gradient(900px 600px at 100% 0%, rgba(99,102,241,.10), transparent 50%),
        linear-gradient(180deg,#f7f9fc 0%,#fbfdff 55%,#f7f9fc 100%);
      color:var(--text);
      font:16px/1.6 system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,'Helvetica Neue',Arial;
    }

    a{color:var(--brand-ink);text-decoration:none}
    a:hover{text-decoration:underline}

    .subpage-hero{
      padding:18px 0 10px;
      border-bottom:1px solid var(--border);
      background:rgba(255,255,255,0.82);
      backdrop-filter:blur(10px);
    }
    .subpage-nav{
      max-width:1100px;
      margin:0 auto;
      padding:0 20px 4px;
    }
    .nav-back{
      font-size:14px;
      color:var(--subtle);
    }

    .hero-content{
      max-width:1100px;
      margin:0 auto;
      padding:10px 20px 24px;
    }
    .eyebrow{
      text-transform:uppercase;
      letter-spacing:.16em;
      font-size:12px;
      color:var(--subtle);
      margin:0 0 4px;
    }
    .hero-content h1{
      margin:0 0 8px;
      font-size:clamp(26px, 4vw, 34px);
      line-height:1.15;
    }
    .hero-lead{
      margin:0 0 10px;
      max-width:60ch;
      color:var(--subtle);
    }

    .pill{
      display:inline-flex;
      align-items:center;
      gap:6px;
      padding:6px 10px;
      font-size:13px;
      border-radius:999px;
      border:1px solid var(--border);
      background:var(--muted);
      color:var(--subtle);
    }
    .hero-tags{
      margin-top:10px;
      display:flex;
      flex-wrap:wrap;
      gap:8px;
    }

    .subpage-main{
      max-width:1100px;
      margin:0 auto 48px;
      padding:20px;
    }
    .subsection{
      background:var(--panel);
      border-radius:18px;
      border:1px solid var(--border);
      box-shadow:var(--shadow);
      padding:20px 22px;
      margin-bottom:20px;
    }
    .subsection h2{
      margin-top:0;
      font-size:20px;
    }

    .metric-grid{
      display:grid;
      grid-template-columns:repeat(auto-fit,minmax(200px,1fr));
      gap:14px;
    }
    .metric{
      padding:12px 12px 10px;
      border-radius:14px;
      background:var(--muted);
    }
    .metric-label{
      font-size:12px;
      text-transform:uppercase;
      letter-spacing:.14em;
      color:var(--subtle);
    }
    .metric-value{
      font-weight:700;
      font-size:16px;
      margin:2px 0 4px;
    }
    .metric-note{
      margin:0;
      font-size:13px;
      color:var(--subtle);
    }

    .numbered{
      padding-left:18px;
    }
    .numbered li + li{
      margin-top:6px;
    }

    .bullet-list{
      padding-left:18px;
      margin:8px 0 0;
    }
    .bullet-list li + li{
      margin-top:4px;
    }

    /* Figure cards for the two images */
    .figure-grid{
      display:grid;
      gap:18px;
    }
    @media (min-width: 900px){
      .figure-grid{
        grid-template-columns:minmax(0,1.1fr) minmax(0,1.1fr);
      }
    }
    .figure-card{
      background:var(--muted);
      border-radius:16px;
      padding:10px 10px 12px;
      border:1px solid var(--border);
      box-shadow:var(--shadow);
    }
    .figure-card img{
      width:100%;
      height:auto;
      border-radius:12px;
      display:block;
      border:1px solid var(--border);
      box-shadow:0 6px 20px rgba(15,23,42,0.18);
    }
    .figure-card figcaption{
      margin-top:8px;
      font-size:13px;
      color:var(--subtle);
    }

    @media (max-width:600px){
      .subpage-main{padding:14px;}
      .subsection{padding:16px 16px 18px;}
    }
  </style>
</head>
<body class="subpage">

  <!-- Simple top nav back to home -->
  <header class="subpage-hero">
    <nav class="subpage-nav">
      <a href="index.html#research" class="nav-back">&larr; Back to overview</a>
    </nav>

    <div class="hero-content">
      <p class="eyebrow">Project</p>
      <h1>Passive Dementia Screening via Facial Temporal Micro-Dynamics</h1>
      <p class="hero-lead">
        Language-free screening from ordinary talking-head video using stabilised
        <strong>facial micro-dynamics</strong> instead of transcripts or scripted interviews.
      </p>

      <div class="hero-tags">
        <span class="pill">Dementia &amp; Cognitive Health</span>
        <span class="pill">Computer Vision</span>
        <span class="pill">Time-series Analysis</span>
        <span class="pill">Explainable Screening</span>
      </div>
    </div>
  </header>

  <main class="section subpage-main">

    <!-- High-level summary -->
    <section class="subsection">
      <h2>What this project does</h2>
      <p>
        This project develops a <strong>passive dementia screening pipeline</strong> that works on short,
        camera-facing talking-head videos. Instead of relying on language, transcripts, or scripted
        clinical interviews, it analyses <strong>how the face moves over time</strong>—blink rhythm,
        eyelid stability, mouth–jaw motion, gaze dispersion, brow asymmetry, and subtle head
        micro-jitter—to estimate dementia risk from natural, in-the-wild recordings.
      </p>
      <p>
        The core idea: if we can reliably quantify small, prosodic-scale facial movements, we can
        build <strong>content-agnostic biomarkers</strong> that transfer across languages, topics, and
        devices, and can later be deployed on humanoid robots or clinical workstations without
        re-recording patients under strict protocols.
      </p>
    </section>

    <!-- Quick project facts -->
    <section class="subsection">
      <h2>Project at a glance</h2>
      <div class="metric-grid">
        <div class="metric">
          <div class="metric-label">Input</div>
          <div class="metric-value">Short talking-head clips</div>
          <p class="metric-note">Single-speaker, camera-facing YouTube-style videos.</p>
        </div>

        <div class="metric">
          <div class="metric-label">Signals</div>
          <div class="metric-value">5 facial micro-dynamics</div>
          <p class="metric-note">
            Blink/eyelid, mouth–jaw, gaze dispersion, brow asymmetry, head micro-jitter.
          </p>
        </div>

        <div class="metric">
          <div class="metric-label">Dataset</div>
          <div class="metric-value">YT-DemTalk (300 clips)</div>
          <p class="metric-note">
            150 self-reported dementia, 150 controls; subject-safe splits.
          </p>
        </div>

        <div class="metric">
          <div class="metric-label">Best performance</div>
          <div class="metric-value">AUROC 0.953</div>
          <p class="metric-note">
            AP 0.961, F1 0.851, Acc 0.857 with a calibrated RF head.
          </p>
        </div>
      </div>
    </section>

    <!-- Pipeline description -->
    <section class="subsection">
      <h2>Pipeline in one pass</h2>
      <p>
        The system turns raw video into a calibrated dementia-risk score through a sequence of
        lightweight, interpretable steps:
      </p>
      <ol class="numbered">
        <li>
          <strong>Stabilise the face.</strong> Each frame is processed with FaceMesh/Iris and head-pose
          estimation (SolvePnP) to decouple camera shake from true micromotor activity.
        </li>
        <li>
          <strong>Extract micro-dynamic streams.</strong> For every frame, the system tracks
          blink and eyelid regularity, mouth–jaw opening cycles, gaze dispersion, brow asymmetry,
          and head micro-movements.
        </li>
        <li>
          <strong>Short-window encoding.</strong> Signals are smoothed and summarised over 6-second
          windows (2-second hop) after a 15-second baseline, capturing prosodic-scale dynamics
          rather than isolated frames.
        </li>
        <li>
          <strong>Motion-mix composition.</strong> Each window is encoded as a <em>conserved-sum motion
          mix</em> (how activity is distributed across streams), then mapped into
          <strong>Aitchison/ILR space</strong> so distances and PCA are geometrically meaningful.
        </li>
        <li>
          <strong>Dimensionality reduction &amp; shallow head.</strong> PCA compresses the ILR features,
          followed by a calibrated <strong>Random Forest</strong> classifier that outputs dementia
          probability.
        </li>
        <li>
          <strong>Calibration &amp; thresholding.</strong> Temperature scaling on the validation set yields
          well-calibrated probabilities; a threshold around <code>τ = 0.636</code> balances high precision
          with useful recall for screening scenarios.
        </li>
      </ol>
    </section>

    <!-- Visual UI + pipeline figures -->
    <section class="subsection">
      <h2>Analysis UI and pipeline visuals</h2>
      <div class="figure-grid">
        <figure class="figure-card">
          <img src="dementia%20(4).png"
               alt="Dashboard showing multiple YT-DemTalk faces with overlayed landmarks and micro-dynamics traces." />
          <figcaption>
            Live analysis UI: multiple YT-DemTalk clips with stabilised landmarks, stream traces
            (blink, jaw, gaze, brow, head), and calibrated risk badges for fast clinician review.
          </figcaption>
        </figure>

        <figure class="figure-card">
          <img src="dementia.diagram%20(6).png"
               alt="Architecture diagram of the passive dementia screening micro-dynamics pipeline." />
          <figcaption>
            End-to-end micro-dynamics pipeline: extraction &amp; event tokens, time+spectral encoding,
            adaptive baseline normalisation, and calibrated stacked classifier head.
          </figcaption>
        </figure>
      </div>
    </section>

    <!-- Dataset section -->
    <section class="subsection">
      <h2>YT-DemTalk: in-the-wild dementia video corpus</h2>
      <p>
        To support passive screening, the project introduces <strong>YT-DemTalk</strong>, a new corpus of
        300 publicly available, camera-facing videos curated from YouTube and similar platforms.
        Recordings include personal vlogs, interviews, and day-in-the-life clips where people
        speak naturally outside experimental or clinical settings.
      </p>
      <ul class="bullet-list">
        <li><strong>Balanced labels:</strong> 150 clips from individuals who explicitly self-report
            a dementia diagnosis; 150 from individuals with no such self-report.</li>
        <li><strong>Ecological validity:</strong> real lighting, framing, cameras, and topics rather
            than lab-style scripted protocols.</li>
        <li><strong>Subject-safe splits:</strong> train/val/test partitions ensure no subject leakage
            across splits.</li>
        <li><strong>Quality gate:</strong> clips with heavy occlusion, severe compression, or unstable
            tracking are removed via a predefined face-quality filter.</li>
      </ul>
      <p>
        The dataset is released as <strong>YT-DemTalk</strong> on Hugging Face:
        <a href="https://huggingface.co/datasets/filo-cenacchi/YT-DemTalk/tree/main" target="_blank" rel="noopener">
          browse YT-DemTalk dataset
        </a>.
      </p>
    </section>

    <!-- Results & interpretation -->
    <section class="subsection">
      <h2>Key results and insights</h2>
      <p>
        On YT-DemTalk, the best configuration—a Stats&nbsp;+&nbsp;PCA&nbsp;+&nbsp;Random Forest head trained
        on motion-mix features—achieves:
      </p>
      <ul class="bullet-list">
        <li><strong>AUROC:</strong> 0.953</li>
        <li><strong>Average Precision (AP):</strong> 0.961</li>
        <li><strong>F1-score:</strong> 0.851</li>
        <li><strong>Accuracy:</strong> 0.857 at the tuned threshold (τ ≈ 0.636)</li>
      </ul>
      <p>
        Single-channel ablations show that <strong>gaze lability</strong> and <strong>mouth/jaw dynamics</strong>
        carry the strongest signal, with brow asymmetry and head micro-jitter providing secondary
        gains. This aligns with neuro-cognitive findings on oculomotor anomalies, orofacial praxis,
        and postural stability in dementia.
      </p>
      <p>
        A live analysis UI overlays stabilised landmarks and stream traces (blink, gaze, jaw, etc.)
        on the video, with calibrated “risk badges” summarising the current estimate. This makes the
        model’s behaviour <strong>visually inspectable</strong> by clinicians: they can see risk rise when
        gaze wanders or jaw cycles become irregular, and fall when facial behaviour is stable and
        well-framed.
      </p>
    </section>

    <!-- Role in broader agenda -->
    <section class="subsection">
      <h2>How this connects to my broader research</h2>
      <p>
        Passive dementia screening is a key building block in my wider agenda on
        <strong>humanoid-AI diagnostic agents</strong> and <strong>avatar-based longitudinal analysis</strong>.
        The micro-dynamics representation is deliberately lightweight and semantics-free, making it
        suitable for:
      </p>
      <ul class="bullet-list">
        <li>
          <strong>On-device deployment</strong> on humanoid robots such as Ameca, where facial
          expressions and camera-facing interaction are already central.
        </li>
        <li>
          <strong>Avatarization pipelines</strong> that replay privacy-preserving 3D avatars driven
          by micro-dynamic descriptors instead of raw video.
        </li>
        <li>
          <strong>Multi-task triage systems</strong> that combine depression, PTSD, and dementia
          signals within a single calibrated decision support dashboard.
        </li>
      </ul>
    </section>

    <!-- Future work -->
    <section class="subsection">
      <h2>Next directions</h2>
      <p>Ongoing and future work focuses on:</p>
      <ul class="bullet-list">
        <li>
          Coupling micro-dynamics with <strong>self-supervised video pretraining</strong> for stronger
          cross-domain robustness.
        </li>
        <li>
          Extending from short clips to <strong>longitudinal stability measures</strong> across multiple
          sessions and views.
        </li>
        <li>
          Embedding the model into <strong>resource-constrained humanoid platforms</strong> for on-device,
          in-the-wild triage with human-in-the-loop oversight and calibration monitoring.
        </li>
      </ul>
    </section>

  </main>
</body>
</html>
