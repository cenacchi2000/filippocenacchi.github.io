<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Passive Dementia Screening – Filippo Cenacchi</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="styles.css" />
</head>
<body class="subpage">

  <!-- Simple top nav back to home -->
  <header class="subpage-hero">
    <nav class="subpage-nav">
      <a href="index.html" class="nav-back">&larr; Back to overview</a>
    </nav>

    <div class="hero-content">
      <p class="eyebrow">Project</p>
      <h1>Passive Dementia Screening via Facial Temporal Micro-Dynamics</h1>
      <p class="hero-lead">
        Language-free screening from ordinary talking-head video using stabilized
        <strong>facial micro-dynamics</strong> instead of transcripts or scripted interviews.
      </p>

      <div class="hero-tags">
        <span class="pill">Dementia &amp; Cognitive Health</span>
        <span class="pill">Computer Vision</span>
        <span class="pill">Time-series Analysis</span>
        <span class="pill">Explainable Screening</span>
      </div>
    </div>
  </header>

  <main class="section subpage-main">

    <!-- High-level summary -->
    <section class="subsection">
      <h2>What this project does</h2>
      <p>
        This project develops a <strong>passive dementia screening pipeline</strong> that works on short,
        camera-facing talking-head videos. Instead of relying on language, transcripts, or scripted
        clinical interviews, it analyses <strong>how the face moves over time</strong>—blink rhythm,
        eyelid stability, mouth–jaw motion, gaze dispersion, brow asymmetry, and subtle head
        micro-jitter—to estimate dementia risk from natural, in-the-wild recordings. :contentReference[oaicite:0]{index=0}
      </p>
      <p>
        The core idea: if we can reliably quantify small, prosodic-scale facial movements, we can
        build <strong>content-agnostic biomarkers</strong> that transfer across languages, topics, and
        devices, and can later be deployed on humanoid robots or clinical workstations without
        re-recording patients under strict protocols.
      </p>
    </section>

    <!-- Quick project facts -->
    <section class="subsection">
      <h2>Project at a glance</h2>
      <div class="metric-grid">
        <div class="metric">
          <div class="metric-label">Input</div>
          <div class="metric-value">Short talking-head clips</div>
          <p class="metric-note">Single-speaker, camera-facing YouTube-style videos.</p>
        </div>

        <div class="metric">
          <div class="metric-label">Signals</div>
          <div class="metric-value">5 facial micro-dynamics</div>
          <p class="metric-note">
            Blink/eyelid, mouth–jaw, gaze dispersion, brow asymmetry, head micro-jitter.
          </p>
        </div>

        <div class="metric">
          <div class="metric-label">Dataset</div>
          <div class="metric-value">YT-DemTalk (300 clips)</div>
          <p class="metric-note">
            150 self-reported dementia, 150 controls; subject-safe splits.
          </p>
        </div>

        <div class="metric">
          <div class="metric-label">Best performance</div>
          <div class="metric-value">AUROC 0.953</div>
          <p class="metric-note">
            AP 0.961, F1 0.851, Acc 0.857 with a calibrated RF head.
          </p>
        </div>
      </div>
    </section>

    <!-- Pipeline description -->
    <section class="subsection">
      <h2>Pipeline in one pass</h2>
      <p>
        The system turns raw video into a calibrated dementia-risk score through a sequence of
        lightweight, interpretable steps:
      </p>
      <ol class="numbered">
        <li>
          <strong>Stabilize the face.</strong> Each frame is processed with FaceMesh/Iris and head-pose
          estimation (SolvePnP) to decouple camera shake from true micromotor activity.
        </li>
        <li>
          <strong>Extract micro-dynamic streams.</strong> For every frame, the system tracks:
          blink and eyelid regularity, mouth–jaw opening cycles, gaze dispersion, brow asymmetry,
          and head micro-movements.
        </li>
        <li>
          <strong>Short-window encoding.</strong> Signals are smoothed and summarized over 6-second
          windows (2-second hop) after a 15-second baseline, capturing prosodic-scale dynamics
          rather than isolated frames.
        </li>
        <li>
          <strong>Motion-mix composition.</strong> Each window is encoded as a <em>conserved-sum motion
          mix</em> (how activity is distributed across streams), then mapped into
          <strong>Aitchison/ILR space</strong> so distances and PCA are geometrically meaningful.
        </li>
        <li>
          <strong>Dimensionality reduction &amp; shallow head.</strong> PCA compresses the ILR features,
          followed by a calibrated <strong>Random Forest</strong> classifier that outputs dementia
          probability.
        </li>
        <li>
          <strong>Calibration &amp; thresholding.</strong> Temperature scaling on the validation set yields
          well-calibrated probabilities; a threshold around <code>τ = 0.636</code> balances high precision
          with useful recall for screening scenarios.
        </li>
      </ol>
    </section>

    <!-- Dataset section -->
    <section class="subsection">
      <h2>YT-DemTalk: in-the-wild dementia video corpus</h2>
      <p>
        To support passive screening, the project introduces <strong>YT-DemTalk</strong>, a new corpus of
        300 publicly available, camera-facing videos curated from YouTube and similar platforms.
        Recordings include personal vlogs, interviews, and day-in-the-life clips where people
        speak naturally outside experimental or clinical settings. :contentReference[oaicite:1]{index=1}
      </p>
      <ul class="bullet-list">
        <li><strong>Balanced labels:</strong> 150 clips from individuals who explicitly self-report
            a dementia diagnosis; 150 from individuals with no such self-report.</li>
        <li><strong>Ecological validity:</strong> real lighting, framing, cameras, and topics rather
            than lab-style scripted protocols.</li>
        <li><strong>Subject-safe splits:</strong> train/val/test partitions ensure no subject leakage
            across splits.</li>
        <li><strong>Quality gate:</strong> clips with heavy occlusion, severe compression, or unstable
            tracking are removed via a predefined face-quality filter.</li>
      </ul>
      <p>
        The dataset is released as <strong>YT-DemTalk</strong> on Hugging Face:
        <a href="https://huggingface.co/datasets/filo-cenacchi/YT-DemTalk/tree/main" target="_blank" rel="noopener">
          browse YT-DemTalk dataset
        </a>.
      </p>
    </section>

    <!-- Results & interpretation -->
    <section class="subsection">
      <h2>Key results and insights</h2>
      <p>
        On YT-DemTalk, the best configuration—a Stats&nbsp;+&nbsp;PCA&nbsp;+&nbsp;Random Forest head trained
        on motion-mix features—achieves:
      </p>
      <ul class="bullet-list">
        <li><strong>AUROC:</strong> 0.953</li>
        <li><strong>Average Precision (AP):</strong> 0.961</li>
        <li><strong>F1-score:</strong> 0.851</li>
        <li><strong>Accuracy:</strong> 0.857 at the tuned threshold (τ ≈ 0.636)</li>
      </ul>
      <p>
        Single-channel ablations show that <strong>gaze lability</strong> and <strong>mouth/jaw dynamics</strong>
        carry the strongest signal, with brow asymmetry and head micro-jitter providing secondary
        gains. This aligns with neuro-cognitive findings on oculomotor anomalies, orofacial praxis,
        and postural stability in dementia. :contentReference[oaicite:2]{index=2}
      </p>
      <p>
        A live analysis UI overlays stabilized landmarks and stream traces (blink, gaze, jaw, etc.)
        on the video, with calibrated “risk badges” summarizing the current estimate. This makes the
        model’s behaviour <strong>visually inspectable</strong> by clinicians: they can see risk rise when
        gaze wanders or jaw cycles become irregular, and fall when facial behaviour is stable and
        well-framed.
      </p>
    </section>

    <!-- Role in your broader agenda -->
    <section class="subsection">
      <h2>How this connects to my broader research</h2>
      <p>
        Passive dementia screening is a key building block in my wider agenda on
        <strong>humanoid-AI diagnostic agents</strong> and <strong>avatar-based longitudinal analysis</strong>.
        The micro-dynamics representation is deliberately lightweight and semantics-free, making it
        suitable for:
      </p>
      <ul class="bullet-list">
        <li>
          <strong>On-device deployment</strong> on humanoid robots such as Ameca, where facial
          expressions and camera-facing interaction are already central.
        </li>
        <li>
          <strong>Avatarization pipelines</strong> that replay privacy-preserving 3D avatars driven
          by micro-dynamic descriptors instead of raw video.
        </li>
        <li>
          <strong>Multi-task triage systems</strong> that combine depression, PTSD, and dementia
          signals within a single calibrated decision support dashboard.
        </li>
      </ul>
    </section>

    <!-- Future work -->
    <section class="subsection">
      <h2>Next directions</h2>
      <p>
        Ongoing and future work focuses on:
      </p>
      <ul class="bullet-list">
        <li>
          Coupling micro-dynamics with <strong>self-supervised video pretraining</strong> for stronger
          cross-domain robustness.
        </li>
        <li>
          Extending from short clips to <strong>longitudinal stability measures</strong> across multiple
          sessions and views.
        </li>
        <li>
          Embedding the model into <strong>resource-constrained humanoid platforms</strong> for on-device,
          in-the-wild triage with human-in-the-loop oversight and calibration monitoring. :contentReference[oaicite:3]{index=3}
        </li>
      </ul>
    </section>

  </main>
</body>
</html>
